{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport re\nimport nltk\nfrom matplotlib import pyplot \nimport pickle","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import backend\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Dense,Input,GRU,LSTM,Embedding, Dropout, Activation \nfrom keras.activations import relu\nfrom keras.models import Sequential ,Model\nfrom keras import initializers, regularizers, constraints, optimizers, layers\nfrom keras.optimizers import Adam \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score,f1_score,roc_auc_score,log_loss,classification_report,confusion_matrix,multilabel_confusion_matrix ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Checkpoint 0"},{"metadata":{"trusted":true},"cell_type":"code","source":"new_data = pd.read_csv('../input/cleansed_data.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_data['annotation'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"count=0\nnew_data['tidy_text']= new_data['tidy_text']\nnew_data['tidy_text']= new_data['tidy_text'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_data['tidy_text']= new_data['tidy_text'].str.lower()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def reduce_lengthening(text):\n    text = text.lower()\n    pattern = re.compile(r\"(.)\\1{2,}\")\n    return pattern.sub(r\"\\1\\1\", str(text))\n\nnew_data['tidy_text'] = new_data['tidy_text'].map(lambda com : reduce_lengthening(com))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Checkpoint 2 new_data"},{"metadata":{"trusted":true},"cell_type":"code","source":"new_dataset = new_data.copy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Checkpoint 3"},{"metadata":{"trusted":true},"cell_type":"code","source":"temp = []\nfor i in range(new_dataset.shape[0]):\n    new_dataset.at[i,'annotation']= int(new_dataset.at[i,'annotation'][1])\n\n#     new_dataset.at[i,'annotation']= 'troll' if (int(new_dataset.at[i,'annotation'][2]) == 1) else 'clean'\nnew_dataset","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_dataset.tidy_text.str.split(expand=True).stack().value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.corpus import stopwords\nstopwords = stopwords.words('russian')\nprint(stopwords)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def stop_words(sen): \n    filtered_sentence = []\n    for w in sen.split():\n        if w in stopwords: \n            filtered_sentence.append(w)\n    return \" \".join(filtered_sentence)\nnew_dataset['tidy_text'] = new_dataset['tidy_text'].map(lambda com : stop_words(com))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_dataset.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_dataset.tidy_text.str.split(expand=True).stack().value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = Tokenizer(num_words=100000,filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',)\ntokenizer.fit_on_texts(new_dataset[\"tidy_text\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with open('tokenizer.pickle', 'wb') as handle:\n    pickle.dump(tokenizer,handle)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = new_dataset['tidy_text']\nX = tokenizer.texts_to_sequences(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vocab_size = len(tokenizer.word_index)+1\nprint(vocab_size)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"maxlen = 100\nX = pad_sequences(X, padding='post', maxlen=maxlen)\nX = np.asarray(X)\ny = new_dataset['annotation']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train , X_val ,y_train, y_val = train_test_split(X,y,test_size=0.2,random_state =42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X_train.shape)\nprint(type(X_train))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_dim = 64\nmodel = Sequential()\nmodel.add(Embedding(input_dim = vocab_size, output_dim = embedding_dim,input_length = maxlen,mask_zero=True))\n\nmodel.add(Dense(256,activation='tanh',kernel_initializer='glorot_uniform',\n    bias_initializer='glorot_uniform',return_sequences=True))\n\n\n\nmodel.add(Dense(32,activation='tanh',kernel_initializer='glorot_uniform',bias_initializer='glorot_uniform'))\n\nmodel.add(Dense(32, activation='relu',kernel_initializer='he_uniform',bias_initializer='he_uniform'))\n\nmodel.add(Dense(1, activation='sigmoid',kernel_initializer='he_uniform',\n    bias_initializer='he_uniform'))\n\nmodel.compile(loss='categorical_crossentropy',optimizer='sgd', metrics=['accuracy'])\n\nprint(model.summary())\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"troll=model.fit(X_train,y_train,validation_data=(X_val,y_val),batch_size=64,epochs=500,verbose=2)\nmodel.save('troll_new01.h5')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(X_train,y_train,validation_data=(X_val,y_val),batch_size=64,epochs=20,verbose=2,callbacks=[reduce_lr])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.save('troll02.h5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.evaluate(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.evaluate(X_val,y_val)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# plot loss during training\npyplot.subplot(211)\npyplot.title('Loss')\npyplot.plot(troll.history['loss'], label='train')\npyplot.plot(troll.history['val_loss'], label='test')\npyplot.legend()\n# plot accuracy during training\npyplot.subplot(212)\npyplot.title('Accuracy')\npyplot.plot(troll.history['accuracy'], label='train')\npyplot.plot(troll.history['val_accuracy'], label='test')\npyplot.legend()\npyplot.show()\n# plot loss during training\npyplot.subplot(211)\npyplot.title('Loss')\npyplot.plot(troll.history['loss'], label='train')\npyplot.plot(troll.history['val_loss'], label='test')\npyplot.legend()\n# plot accuracy during training\npyplot.subplot(212)\npyplot.title('Accuracy')\npyplot.plot(troll.history['accuracy'], label='train')\npyplot.plot(troll.history['val_accuracy'], label='test')\npyplot.legend()\npyplot.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.save_weights('troll_new02_weights.h5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.externals import joblib \n  \njoblib.dump(model, 'trollpickle.pkl') ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import load_model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"troll = load_model('troll_new02.h5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"troll.load_weights('troll02_weights.h5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"troll.predict(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"}},"nbformat":4,"nbformat_minor":4}